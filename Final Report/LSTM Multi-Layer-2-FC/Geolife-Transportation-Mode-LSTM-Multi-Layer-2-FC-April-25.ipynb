{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from scipy import linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading cleaned csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User i is data_1$[i]$ in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [0] * 57\n",
    "for i in range(len(data_1)):\n",
    "    fnames = glob.glob('labeled csv Geolife/'+str(i)+'/*.csv')\n",
    "    data_1[i] = np.array([np.loadtxt(f, delimiter=',')[1:] for f in fnames])\n",
    "data_1 = np.array(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3572"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users are stacked together in data_2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = []\n",
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "for f in fnames:\n",
    "    data_2.append(np.loadtxt(f, delimiter=',')[1:])\n",
    "data_2 = np.array(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40392)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_2[i]) for i in range(len(data_2))])\n",
    "min(A), max(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing segments with length less than 1e-10 because of numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3 = [0] * len(data_2)\n",
    "h = 1e-10\n",
    "c = 0\n",
    "for i in range(len(data_2)):\n",
    "    p1 = data_2[i][:-1]\n",
    "    p2 = data_2[i][1:]\n",
    "    L = ((p2[:,:2]-p1[:,:2])*(p2[:,:2]-p1[:,:2])).sum(axis =1)\n",
    "    I = np.where(L > h)[0]\n",
    "    J = np.where(L < h)[0]\n",
    "    if len(J) > 0:\n",
    "        c += 1\n",
    "    p1 = p1[I]\n",
    "    p2 = p2[I]\n",
    "    if len(I) == 0:\n",
    "        print(i)\n",
    "    gamma = np.concatenate((p1, p2[-1].reshape(1,4)), 0) \n",
    "    if len(gamma) > 0:\n",
    "        data_3[i] = gamma\n",
    "    data_3[i] = np.array(data_3[i])\n",
    "data_3 = np.array(data_3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 40323, array([1380]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_3[i]) for i in range(len(data_3))])\n",
    "min(A), max(A), np.where(A > 40000)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning trajectories to less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08333337376825511, 2085.283333301777, (array([2940]),))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 24 * 60 * (days_date('1899/12/30 2:50:06') - days_date('1899/12/30 2:20:06')) == 20 min\n",
    "Time = np.zeros(len(data_3))\n",
    "for i in range(len(data_3)):\n",
    "    Time[i] = 24 * 60 * (data_3[i][-1][2] - data_3[i][0][2]) # = 20 minutes \n",
    "min(Time), max(Time), np.where(Time>2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.where(Time>20)[0]\n",
    "len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(trajectory):\n",
    "    trajectories = []\n",
    "    a = 24 * 60 * (trajectory[-1][2] - trajectory[0][2])\n",
    "    if a <= 20:\n",
    "        return np.array(trajectory.reshape(1, len(trajectory), 4))\n",
    "    else: \n",
    "        i = 0\n",
    "        while a > 20:\n",
    "            j = i + 0\n",
    "            val = 0\n",
    "            while val < 20: \n",
    "                if i < len(trajectory) - 1:\n",
    "                    temp = val + 0\n",
    "                    val += 24 * 60 * (trajectory[:,2][1:][i] - trajectory[:,2][:-1][i])\n",
    "                    i += 1\n",
    "                else: \n",
    "                    break\n",
    "            if len(trajectory[j:i-1]) > 0:\n",
    "                trajectories.append(trajectory[j:i-1])\n",
    "            a = a - val\n",
    "        if len(trajectory[i:]) > 0:\n",
    "            trajectories.append(trajectory[i:])\n",
    "    trajectories = np.array(trajectories)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if partitioning into less than 20 minutes worked correctly\n",
    "for j in J:\n",
    "    A = partition(data_3[j])\n",
    "    B = np.array([24 * 60 * sum(A[i][:,2][1:] - A[i][:,2][:-1]) for i in range(len(A))])\n",
    "    I = np.where(B > 20)[0]\n",
    "    if len(I) > 0: \n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_4 below is the array of trajectories having less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = []\n",
    "for i in range(len(data_3)):\n",
    "    A = partition(data_3[i])\n",
    "    for j in range(len(A)):\n",
    "        data_4.append(A[j])\n",
    "data_4 = np.array(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11833,), (360, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4.shape, data_4[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.where(np.array([len(data_4[i]) for i in range(len(data_4))]) != 1)[0]\n",
    "data_4 = data_4[I]\n",
    "len(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int1 = np.vectorize(int)\n",
    "data_5 = []\n",
    "c = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) < 2: \n",
    "        data_5.append(data_4[i])\n",
    "        c += 1\n",
    "data_5 = np.array(data_5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_6 = []\n",
    "d = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) == 2: \n",
    "        data_6.append(data_4[i])\n",
    "        d += 1\n",
    "data_6 = np.array(data_6)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a:b\n",
    "# a is the number of labels in a trajectory\n",
    "# b is the number of trajectries with a labels\n",
    "D = {1:10121, 2:1671, 3:39, 4:2, 5:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modes = ['walk', 'bike', 'bus', 'driving', 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trajectories of length 1 with label 0, 1, 2, 3, 4: [3383, 1650, 1929, 2214, 863]\n"
     ]
    }
   ],
   "source": [
    "C = []\n",
    "for j in range(5):\n",
    "    c = 0\n",
    "    for i in range(len(data_5)):\n",
    "        if data_5[i][0][-1] == j:\n",
    "            c += 1\n",
    "    C.append(c)\n",
    "print(\"number of trajectories of length 1 with label 0, 1, 2, 3, 4:\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating trajectories with 3, 4, 5, 6 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing length 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1671, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_7 = [0] * len(data_6)\n",
    "for i in range(len(data_6)):\n",
    "    I = list(set(data_6[i][:,3]))\n",
    "    data_7[i] = []\n",
    "    J1 = np.where(data_6[i][:,3] == I[0])\n",
    "    J2 = np.where(data_6[i][:,3] == I[1])\n",
    "    D1 = data_6[i][J1]\n",
    "    D2 = data_6[i][J2]\n",
    "    data_7[i].append(D1)\n",
    "    data_7[i].append(D2)\n",
    "data_7 = np.array(data_7)\n",
    "data_7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentences of length 3, 4, 5, 6 from 10039 length 1 trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "n_1 = 1000 # number of length 1 sentences\n",
    "n_2 = 1000 # len(data_7)# 1671: number of length 2 sentences\n",
    "n_3 = 800 # number of length 3 sentences\n",
    "n_4 = 700 # number of length 4 sentences\n",
    "n_5 = 1900 # number of length 5 sentences\n",
    "#n_6 = 1000 # number of length 6 sentences\n",
    "\n",
    "for i in range(n_1):\n",
    "    I = np.random.randint(0, 10039, size=1)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_2): \n",
    "    data.append(data_7[i])\n",
    "\n",
    "for i in range(n_3):\n",
    "    I = np.random.randint(0, 10039, size=3)\n",
    "    data.append(data_5[I])\n",
    "\n",
    "for i in range(n_4):\n",
    "    I = np.random.randint(0, 10039, size=4)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_5):\n",
    "    I = np.random.randint(0, 10039, size=5)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "#for i in range(n_6):\n",
    "#    I = np.random.randint(0, 10039, size=6)\n",
    "#    data.append(data_5[I])\n",
    "    \n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (2,) (3,) (4,) (5,)\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape, data[n_1].shape, data[n_1+n_2].shape, data[n_1+n_2+n_3].shape, \n",
    "      data[n_1+n_2+n_3+n_4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed for CMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "source": [
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "### $\\phi_1(x, y) = (\\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{4n}$\n",
    "\n",
    "### $\\phi_2(x, y) = (\\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{8n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping1(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping2(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature mapping using landmarks\n",
    "\n",
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "#### Now we would like to use a feature mapping introduced in the following paper:\n",
    "\n",
    "``Jeff M. Phillips and Pingfan Tang. Simple distances for trajectories via landmarks. In ACM GIS SIGSPATIAL, 2019.''\n",
    "\n",
    "Following the paper, let $q \\in \\mathbb{R}^2$ be a landmark and $\\gamma$ be a trajectory in $\\mathbb{R}^2$. We define \n",
    "$$v_q(\\gamma) = {\\rm dist}(\\gamma, q) = min_{p \\in \\gamma} \\|q - p\\|_2.$$\n",
    "\n",
    "We randomly choose $m$ (here $m=20$ will be used) landmaks in $\\mathbb{R}^2$ around trajectories and call them $Q$, so $Q=\\{q_1, q_2, \\ldots, q_m\\}$. Then we define the feature mapping $v_Q$ by \n",
    "$$v_Q(\\gamma) = (v_{q_1}(\\gamma), v_{q_2}(\\gamma), \\ldots, v_{q_m}(\\gamma)) \\in \\mathbb{R}^m.$$\n",
    "\n",
    "Then we combine this feature mapping with $\\phi_1$ and $\\phi_2$ to get the following feature mappings:\n",
    "\n",
    "### $\\phi_3(x, y) = (v_Q(x_i), y_i)_{i=1}^n \\in \\mathbb{R}^{(m+1)n}$\n",
    "\n",
    "### $\\phi_4(x, y) = (v_Q(x_i), \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+4)n}$\n",
    "\n",
    "### $\\phi_5(x, y) = (v_Q(x_i), \\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+8)n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark Feature Mapping $v_Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap_v_Q(Q, gamma):\n",
    "    \n",
    "    p2 = gamma[1:]\n",
    "    p1 = gamma[:-1]\n",
    "    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))\n",
    "    II = np.where(L>10e-8)[0]\n",
    "    L = L[II]\n",
    "    p1 = p1[II]\n",
    "    p2 = p2[II]\n",
    "    w = (p1-p2)*(-1,1)/(L*np.ones((2,1))).T\n",
    "    w[:,[0, 1]] = w[:,[1, 0]]\n",
    "    \n",
    "    dist_dot = np.sum(w * (Q.reshape(len(Q),1,2) - p1), axis=2)\n",
    "    \n",
    "    x = abs(dist_dot.copy())\n",
    "    R = (L**2).reshape(-1,1)\n",
    "    u = p1 + ((((np.sum(((Q.reshape(len(Q),1,2) - p1) * (p2 - p1)),axis=2).reshape(len(Q)\n",
    "                ,-1,1,1) * (p2-p1).reshape(len(p2-p1),1,2))).reshape(len(Q),len(p1),2))/R)\n",
    "    \n",
    "    G = np.sqrt(np.sum((u-p1)*(u-p1), axis=2))\n",
    "    H = np.sqrt(np.sum((u-p2)*(u-p2), axis=2))\n",
    "    d1 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p1)*(Q.reshape(len(Q),1,2)-p1), axis=2))\n",
    "    d2 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p2)*(Q.reshape(len(Q),1,2)-p2), axis=2))\n",
    "\n",
    "    dist = np.where(abs(G + H - L) < np.ones(len(L)) * (10e-8), x, np.minimum(d1, d2))\n",
    "\n",
    "    j = np.argmin(dist, axis =1)\n",
    "    dist_weighted = dist[np.arange(len(dist)),j]\n",
    "    \n",
    "    return dist_weighted.reshape(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureMapping3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping3(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping4(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping5(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(data):\n",
    "    I = []\n",
    "    random.shuffle(data)\n",
    "    for j in range(1,6):\n",
    "        I.append(np.where([len(data[i])==j for i in range(len(Data))])[0])\n",
    "\n",
    "    train = np.concatenate((data[I[0]][len(I[0])//3:], data[I[1]][len(I[1])//3:], \n",
    "                            data[I[2]][len(I[2])//3:], data[I[3]][len(I[3])//3:], \n",
    "                            data[I[4]][len(I[4])//3:]), 0)\n",
    "\n",
    "    test = np.concatenate((data[I[0]][:len(I[0])//3], data[I[1]][:len(I[1])//3],\n",
    "                           data[I[2]][:len(I[2])//3], data[I[3]][:len(I[3])//3],\n",
    "                           data[I[4]][:len(I[4])//3]), 0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for feeding to a clssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localModel(data):\n",
    "    X = [0] * 5 # 5 is the length of label set\n",
    "    X[0] = []\n",
    "    for i in range(len(data)):\n",
    "        X[0].append(data[i][0])\n",
    "    X[0] = np.array(X[0])\n",
    "    for j in range(1, 5):\n",
    "        X[j] = []\n",
    "        I = np.where([len(data[i]) > j for i in range(len(data))])[0]\n",
    "        for i in I:\n",
    "            X[j].append(np.insert(data[i][j], len(data[i][j])-1, data[i][j-1][-1], axis=0))\n",
    "        X[j] = np.array(X[j])\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple layers and 2 FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-layer LSTM with 2 FC layers\n",
    "class LSTM_multi_fc(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, num_layers, num_classes):\n",
    "\n",
    "        super(LSTM_multi_fc, self).__init__()\n",
    "        \n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim_1, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(hidden_dim_2, num_classes)\n",
    "\n",
    "    def forward(self, traj_span):\n",
    "\n",
    "        h0 = torch.zeros((self.num_layers, traj_span.size(0), self.hidden_dim_1))\n",
    "        c0 = torch.zeros((self.num_layers, traj_span.size(0), self.hidden_dim_1))\n",
    "        lstm_out, hidden = self.lstm(traj_span.view(len(traj_span), 1, -1), (h0, c0))\n",
    "        out = self.relu(lstm_out.view(len(traj_span), -1)) # [seqence_length, hidden_dim_2]\n",
    "        out = self.fc1(out) \n",
    "        out = self.relu(out)\n",
    "        outputs = self.fc2(out.view(len(traj_span), -1)) # [seqence_length, num_classes]\n",
    "        scores = F.log_softmax(outputs, dim=1) # [seqence_length, num_classes]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and testing\n",
    "\n",
    "The following function first choses a model, then trains on train data and finally \n",
    "evaluates the trained model on test data and outputs the accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_data, test_data, hidden_dim_1=20, hidden_dim_2=10, num_classes=5, \n",
    "             num_layers=1, learning_rate=0.01, n_epochs=100, d=10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    input_dim = len(train_data[0][0][0])\n",
    "    \n",
    "    model = LSTM_multi_fc(input_dim, hidden_dim_1, hidden_dim_2, num_layers, num_classes)\n",
    "\n",
    "    # loss and optimizer\n",
    "    loss_function = nn.NLLLoss() # nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"loss_function=\", loss_function)\n",
    "    print(\"model=\", model)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for sentence, tags in train_data:\n",
    "\n",
    "            # zero the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward pass to get scores\n",
    "            tag_scores = model(sentence)\n",
    "\n",
    "            # computing the loss and gradients \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # updating the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        if((epoch+1)%d == 0):\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train)))\n",
    "        \n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for trajs, labels in train_data:\n",
    "        outputs = model(trajs)\n",
    "        n_samples += len(labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100 * n_correct/n_samples\n",
    "    print(\"n_samples=\", n_samples)\n",
    "    print(\"n_correct=\", n_correct)\n",
    "    print(f'train accuracy: {acc}')\n",
    "    \n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for trajs, labels in test_data:\n",
    "        outputs = model(trajs)\n",
    "        n_samples += len(labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100 * n_correct/n_samples\n",
    "    print(\"n_samples=\", n_samples)\n",
    "    print(\"n_correct=\", n_correct)\n",
    "    print(f'test accuracy: {acc}')\n",
    "    print(time.time() - start_time)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing 20 landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, c = np.min([np.min([np.min(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "  \n",
    "b, d = np.max([np.max([np.max(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Mean = np.mean([np.mean([np.mean(data[i][k][:,:2], axis=0) for k in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Std = np.std([np.std([np.std(data[i][l][:,:2], axis=0) for l in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "m = 20\n",
    "Q = np.ones((m,2))\n",
    "\n",
    "Q[:,0] = np.random.normal(Mean[0], 100*Std[0], m)\n",
    "Q[:,1] = np.random.normal(Mean[1], 20*Std[1], m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.45776047, 114.19543237],\n",
       "       [ 34.80638492, 114.07043061],\n",
       "       [ 34.17283853, 114.33035869],\n",
       "       [ 37.7670618 , 114.17781355],\n",
       "       [ 36.76044097, 114.58316874],\n",
       "       [ 44.04449441, 114.39685861],\n",
       "       [ 38.63531477, 114.26437843],\n",
       "       [ 30.32256267, 114.67661885],\n",
       "       [ 39.59461777, 114.71488658],\n",
       "       [ 36.56722503, 113.95731903],\n",
       "       [ 39.85818948, 114.30191596],\n",
       "       [ 41.09600728, 114.51265944],\n",
       "       [ 40.13351418, 114.32962686],\n",
       "       [ 40.38632773, 114.32322126],\n",
       "       [ 47.31702705, 114.25603823],\n",
       "       [ 35.3250506 , 114.62969867],\n",
       "       [ 43.79412114, 114.28062528],\n",
       "       [ 39.57812802, 114.76865393],\n",
       "       [ 35.99103921, 114.40673065],\n",
       "       [ 34.2859458 , 114.05440932]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_1$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping1(data)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= CrossEntropyLoss()\n",
      "model= LSTM_multi_fc(\n",
      "  (lstm): LSTM(3, 10, batch_first=True)\n",
      "  (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 0.98994\n",
      "Epoch: 20, loss: 0.98119\n",
      "Epoch: 30, loss: 0.97652\n",
      "Epoch: 40, loss: 0.97302\n",
      "Epoch: 50, loss: 0.97077\n",
      "Epoch: 60, loss: 0.97033\n",
      "Epoch: 70, loss: 0.96955\n",
      "Epoch: 80, loss: 0.96876\n",
      "Epoch: 90, loss: 0.96819\n",
      "Epoch: 100, loss: 0.96730\n",
      "n_samples= 11701\n",
      "n_correct= 7468\n",
      "train accuracy: 63.82360482010085\n",
      "n_samples= 5843\n",
      "n_correct= 3667\n",
      "test accuracy: 62.75885675166867\n",
      "257.1407878398895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.75885675166867"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim_1=10, hidden_dim_2=10, num_classes=5, num_layers=1, \n",
    "           learning_rate=0.02, n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_2$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping2(data)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= CrossEntropyLoss()\n",
      "model= LSTM_multi_fc(\n",
      "  (lstm): LSTM(7, 100, batch_first=True)\n",
      "  (fc1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.46483\n",
      "Epoch: 20, loss: 1.47636\n",
      "Epoch: 30, loss: 1.48455\n",
      "Epoch: 40, loss: 1.47754\n",
      "Epoch: 50, loss: 1.46026\n",
      "Epoch: 60, loss: 1.42423\n",
      "Epoch: 70, loss: 1.36048\n",
      "Epoch: 80, loss: 1.31183\n",
      "Epoch: 90, loss: 1.25471\n",
      "Epoch: 100, loss: 1.26979\n",
      "n_samples= 11701\n",
      "n_correct= 5955\n",
      "train accuracy: 50.893086061020426\n",
      "n_samples= 5843\n",
      "n_correct= 3052\n",
      "test accuracy: 52.2334417251412\n",
      "297.5487108230591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.2334417251412"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim_1=100, hidden_dim_2=200, num_classes=5, num_layers=1, \n",
    "           learning_rate=0.005, n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_3$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping3(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= CrossEntropyLoss()\n",
      "model= LSTM_multi_fc(\n",
      "  (lstm): LSTM(20, 20, batch_first=True)\n",
      "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.46177\n",
      "Epoch: 20, loss: 1.45201\n",
      "Epoch: 30, loss: 1.45555\n",
      "Epoch: 40, loss: 1.45349\n",
      "Epoch: 50, loss: 1.44602\n",
      "Epoch: 60, loss: 1.44339\n",
      "Epoch: 70, loss: 1.43888\n",
      "Epoch: 80, loss: 1.43503\n",
      "Epoch: 90, loss: 1.42695\n",
      "Epoch: 100, loss: 1.42242\n",
      "n_samples= 11701\n",
      "n_correct= 4301\n",
      "train accuracy: 36.75754209041962\n",
      "n_samples= 5843\n",
      "n_correct= 2193\n",
      "test accuracy: 37.53208967995892\n",
      "244.36490535736084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.53208967995892"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim_1=20, hidden_dim_2=20, num_classes=5, num_layers=1, \n",
    "           learning_rate=0.01, n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_4$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping4(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= CrossEntropyLoss()\n",
      "model= LSTM_multi_fc(\n",
      "  (lstm): LSTM(23, 100, batch_first=True)\n",
      "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=50, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.06039\n",
      "Epoch: 20, loss: 0.97521\n",
      "Epoch: 30, loss: 0.95581\n",
      "Epoch: 40, loss: 0.94849\n",
      "Epoch: 50, loss: 0.94244\n",
      "Epoch: 60, loss: 0.94214\n",
      "Epoch: 70, loss: 0.93757\n",
      "Epoch: 80, loss: 0.93347\n",
      "Epoch: 90, loss: 0.92923\n",
      "Epoch: 100, loss: 0.92909\n",
      "n_samples= 11701\n",
      "n_correct= 7676\n",
      "train accuracy: 65.60123066404581\n",
      "n_samples= 5843\n",
      "n_correct= 3795\n",
      "test accuracy: 64.94951223686462\n",
      "292.9914376735687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.94951223686462"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim_1=100, hidden_dim_2=50, num_classes=5, num_layers=1, \n",
    "           learning_rate=0.001, n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_5$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = featureMapping5(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())\n",
    "    \n",
    "len(train[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= CrossEntropyLoss()\n",
      "model= LSTM_multi_fc(\n",
      "  (lstm): LSTM(27, 200, batch_first=True)\n",
      "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.15720\n",
      "Epoch: 20, loss: 1.10307\n",
      "Epoch: 30, loss: 1.05697\n",
      "Epoch: 40, loss: 1.03074\n",
      "Epoch: 50, loss: 1.00954\n",
      "Epoch: 60, loss: 1.00050\n",
      "Epoch: 70, loss: 1.00756\n",
      "Epoch: 80, loss: 1.00250\n",
      "Epoch: 90, loss: 0.98932\n",
      "Epoch: 100, loss: 0.99626\n",
      "n_samples= 11701\n",
      "n_correct= 7120\n",
      "train accuracy: 60.84950004273139\n",
      "n_samples= 5843\n",
      "n_correct= 3486\n",
      "test accuracy: 59.66113297963375\n",
      "427.3584179878235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59.66113297963375"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim_1=200, hidden_dim_2=100, num_classes=5, num_layers=1, \n",
    "           learning_rate=0.0015, n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
