{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from scipy import linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading cleaned csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User i is data_1$[i]$ in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [0] * 57\n",
    "for i in range(len(data_1)):\n",
    "    fnames = glob.glob('labeled csv Geolife/'+str(i)+'/*.csv')\n",
    "    data_1[i] = np.array([np.loadtxt(f, delimiter=',')[1:] for f in fnames])\n",
    "data_1 = np.array(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3572"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users are stacked together in data_2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = []\n",
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "for f in fnames:\n",
    "    data_2.append(np.loadtxt(f, delimiter=',')[1:])\n",
    "data_2 = np.array(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40392)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_2[i]) for i in range(len(data_2))])\n",
    "min(A), max(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing segments with length less than 1e-10 because of numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3 = [0] * len(data_2)\n",
    "h = 1e-10\n",
    "c = 0\n",
    "for i in range(len(data_2)):\n",
    "    p1 = data_2[i][:-1]\n",
    "    p2 = data_2[i][1:]\n",
    "    L = ((p2[:,:2]-p1[:,:2])*(p2[:,:2]-p1[:,:2])).sum(axis =1)\n",
    "    I = np.where(L > h)[0]\n",
    "    J = np.where(L < h)[0]\n",
    "    if len(J) > 0:\n",
    "        c += 1\n",
    "    p1 = p1[I]\n",
    "    p2 = p2[I]\n",
    "    if len(I) == 0:\n",
    "        print(i)\n",
    "    gamma = np.concatenate((p1, p2[-1].reshape(1,4)), 0) \n",
    "    if len(gamma) > 0:\n",
    "        data_3[i] = gamma\n",
    "    data_3[i] = np.array(data_3[i])\n",
    "data_3 = np.array(data_3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 40323, array([1380]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_3[i]) for i in range(len(data_3))])\n",
    "min(A), max(A), np.where(A > 40000)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning trajectories to less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08333337376825511, 2085.283333301777, (array([2940]),))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 24 * 60 * (days_date('1899/12/30 2:50:06') - days_date('1899/12/30 2:20:06')) == 20 min\n",
    "Time = np.zeros(len(data_3))\n",
    "for i in range(len(data_3)):\n",
    "    Time[i] = 24 * 60 * (data_3[i][-1][2] - data_3[i][0][2]) # = 20 minutes \n",
    "min(Time), max(Time), np.where(Time>2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.where(Time>20)[0]\n",
    "len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(trajectory):\n",
    "    trajectories = []\n",
    "    a = 24 * 60 * (trajectory[-1][2] - trajectory[0][2])\n",
    "    if a <= 20:\n",
    "        return np.array(trajectory.reshape(1, len(trajectory), 4))\n",
    "    else: \n",
    "        i = 0\n",
    "        while a > 20:\n",
    "            j = i + 0\n",
    "            val = 0\n",
    "            while val < 20: \n",
    "                if i < len(trajectory) - 1:\n",
    "                    temp = val + 0\n",
    "                    val += 24 * 60 * (trajectory[:,2][1:][i] - trajectory[:,2][:-1][i])\n",
    "                    i += 1\n",
    "                else: \n",
    "                    break\n",
    "            if len(trajectory[j:i-1]) > 0:\n",
    "                trajectories.append(trajectory[j:i-1])\n",
    "            a = a - val\n",
    "        if len(trajectory[i:]) > 0:\n",
    "            trajectories.append(trajectory[i:])\n",
    "    trajectories = np.array(trajectories)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if partitioning into less than 20 minutes worked correctly\n",
    "for j in J:\n",
    "    A = partition(data_3[j])\n",
    "    B = np.array([24 * 60 * sum(A[i][:,2][1:] - A[i][:,2][:-1]) for i in range(len(A))])\n",
    "    I = np.where(B > 20)[0]\n",
    "    if len(I) > 0: \n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_4 below is the array of trajectories having less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = []\n",
    "for i in range(len(data_3)):\n",
    "    A = partition(data_3[i])\n",
    "    for j in range(len(A)):\n",
    "        data_4.append(A[j])\n",
    "data_4 = np.array(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11833,), (360, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4.shape, data_4[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.where(np.array([len(data_4[i]) for i in range(len(data_4))]) != 1)[0]\n",
    "data_4 = data_4[I]\n",
    "len(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int1 = np.vectorize(int)\n",
    "data_5 = []\n",
    "c = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) < 2: \n",
    "        data_5.append(data_4[i])\n",
    "        c += 1\n",
    "data_5 = np.array(data_5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_6 = []\n",
    "d = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) == 2: \n",
    "        data_6.append(data_4[i])\n",
    "        d += 1\n",
    "data_6 = np.array(data_6)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a:b\n",
    "# a is the number of labels in a trajectory\n",
    "# b is the number of trajectries with a labels\n",
    "D = {1:10121, 2:1671, 3:39, 4:2, 5:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modes = ['walk', 'bike', 'bus', 'driving', 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trajectories of length 1 with label 0, 1, 2, 3, 4: [3383, 1650, 1929, 2214, 863]\n"
     ]
    }
   ],
   "source": [
    "C = []\n",
    "for j in range(5):\n",
    "    c = 0\n",
    "    for i in range(len(data_5)):\n",
    "        if data_5[i][0][-1] == j:\n",
    "            c += 1\n",
    "    C.append(c)\n",
    "print(\"number of trajectories of length 1 with label 0, 1, 2, 3, 4:\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating trajectories with 3, 4, 5, 6 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing length 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1671, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_7 = [0] * len(data_6)\n",
    "for i in range(len(data_6)):\n",
    "    I = list(set(data_6[i][:,3]))\n",
    "    data_7[i] = []\n",
    "    J1 = np.where(data_6[i][:,3] == I[0])\n",
    "    J2 = np.where(data_6[i][:,3] == I[1])\n",
    "    D1 = data_6[i][J1]\n",
    "    D2 = data_6[i][J2]\n",
    "    data_7[i].append(D1)\n",
    "    data_7[i].append(D2)\n",
    "data_7 = np.array(data_7)\n",
    "data_7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentences of length 3, 4, 5, 6 from 10039 length 1 trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "n_1 = 1000 # number of length 1 sentences\n",
    "n_2 = 1000 # len(data_7)# 1671: number of length 2 sentences\n",
    "n_3 = 800 # number of length 3 sentences\n",
    "n_4 = 700 # number of length 4 sentences\n",
    "n_5 = 1900 # number of length 5 sentences\n",
    "#n_6 = 1000 # number of length 6 sentences\n",
    "\n",
    "for i in range(n_1):\n",
    "    I = np.random.randint(0, 10039, size=1)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_2): \n",
    "    data.append(data_7[i])\n",
    "\n",
    "for i in range(n_3):\n",
    "    I = np.random.randint(0, 10039, size=3)\n",
    "    data.append(data_5[I])\n",
    "\n",
    "for i in range(n_4):\n",
    "    I = np.random.randint(0, 10039, size=4)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_5):\n",
    "    I = np.random.randint(0, 10039, size=5)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "#for i in range(n_6):\n",
    "#    I = np.random.randint(0, 10039, size=6)\n",
    "#    data.append(data_5[I])\n",
    "    \n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (2,) (3,) (4,) (5,)\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape, data[n_1].shape, data[n_1+n_2].shape, data[n_1+n_2+n_3].shape, \n",
    "      data[n_1+n_2+n_3+n_4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed for CMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "source": [
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "### $\\phi_1(x, y) = (\\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{4n}$\n",
    "\n",
    "### $\\phi_2(x, y) = (\\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{8n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping1(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping2(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature mapping using landmarks\n",
    "\n",
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "#### Now we would like to use a feature mapping introduced in the following paper:\n",
    "\n",
    "``Jeff M. Phillips and Pingfan Tang. Simple distances for trajectories via landmarks. In ACM GIS SIGSPATIAL, 2019.''\n",
    "\n",
    "Following the paper, let $q \\in \\mathbb{R}^2$ be a landmark and $\\gamma$ be a trajectory in $\\mathbb{R}^2$. We define \n",
    "$$v_q(\\gamma) = {\\rm dist}(\\gamma, q) = min_{p \\in \\gamma} \\|q - p\\|_2.$$\n",
    "\n",
    "We randomly choose $m$ (here $m=20$ will be used) landmaks in $\\mathbb{R}^2$ around trajectories and call them $Q$, so $Q=\\{q_1, q_2, \\ldots, q_m\\}$. Then we define the feature mapping $v_Q$ by \n",
    "$$v_Q(\\gamma) = (v_{q_1}(\\gamma), v_{q_2}(\\gamma), \\ldots, v_{q_m}(\\gamma)) \\in \\mathbb{R}^m.$$\n",
    "\n",
    "Then we combine this feature mapping with $\\phi_1$ and $\\phi_2$ to get the following feature mappings:\n",
    "\n",
    "### $\\phi_3(x, y) = (v_Q(x_i), y_i)_{i=1}^n \\in \\mathbb{R}^{(m+1)n}$\n",
    "\n",
    "### $\\phi_4(x, y) = (v_Q(x_i), \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+4)n}$\n",
    "\n",
    "### $\\phi_5(x, y) = (v_Q(x_i), \\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+8)n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark Feature Mapping $v_Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap_v_Q(Q, gamma):\n",
    "    \n",
    "    p2 = gamma[1:]\n",
    "    p1 = gamma[:-1]\n",
    "    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))\n",
    "    II = np.where(L>10e-8)[0]\n",
    "    L = L[II]\n",
    "    p1 = p1[II]\n",
    "    p2 = p2[II]\n",
    "    w = (p1-p2)*(-1,1)/(L*np.ones((2,1))).T\n",
    "    w[:,[0, 1]] = w[:,[1, 0]]\n",
    "    \n",
    "    dist_dot = np.sum(w * (Q.reshape(len(Q),1,2) - p1), axis=2)\n",
    "    \n",
    "    x = abs(dist_dot.copy())\n",
    "    R = (L**2).reshape(-1,1)\n",
    "    u = p1 + ((((np.sum(((Q.reshape(len(Q),1,2) - p1) * (p2 - p1)),axis=2).reshape(len(Q)\n",
    "                ,-1,1,1) * (p2-p1).reshape(len(p2-p1),1,2))).reshape(len(Q),len(p1),2))/R)\n",
    "    \n",
    "    G = np.sqrt(np.sum((u-p1)*(u-p1), axis=2))\n",
    "    H = np.sqrt(np.sum((u-p2)*(u-p2), axis=2))\n",
    "    d1 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p1)*(Q.reshape(len(Q),1,2)-p1), axis=2))\n",
    "    d2 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p2)*(Q.reshape(len(Q),1,2)-p2), axis=2))\n",
    "\n",
    "    dist = np.where(abs(G + H - L) < np.ones(len(L)) * (10e-8), x, np.minimum(d1, d2))\n",
    "\n",
    "    j = np.argmin(dist, axis =1)\n",
    "    dist_weighted = dist[np.arange(len(dist)),j]\n",
    "    \n",
    "    return dist_weighted.reshape(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureMapping3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping3(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping4(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping5(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(data):\n",
    "    I = []\n",
    "    random.shuffle(data)\n",
    "    for j in range(1,6):\n",
    "        I.append(np.where([len(data[i])==j for i in range(len(Data))])[0])\n",
    "\n",
    "    train = np.concatenate((data[I[0]][len(I[0])//3:], data[I[1]][len(I[1])//3:], \n",
    "                            data[I[2]][len(I[2])//3:], data[I[3]][len(I[3])//3:], \n",
    "                            data[I[4]][len(I[4])//3:]), 0)\n",
    "\n",
    "    test = np.concatenate((data[I[0]][:len(I[0])//3], data[I[1]][:len(I[1])//3],\n",
    "                           data[I[2]][:len(I[2])//3], data[I[3]][:len(I[3])//3],\n",
    "                           data[I[4]][:len(I[4])//3]), 0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, c = np.min([np.min([np.min(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "  \n",
    "b, d = np.max([np.max([np.max(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Mean = np.mean([np.mean([np.mean(data[i][k][:,:2], axis=0) for k in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Std = np.std([np.std([np.std(data[i][l][:,:2], axis=0) for l in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "m = 20\n",
    "Q = np.ones((m,2))\n",
    "\n",
    "Q[:,0] = np.random.normal(Mean[0], 100*Std[0], m)\n",
    "Q[:,1] = np.random.normal(Mean[1], 20*Std[1], m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-layer LSTM with 1 FC layer\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # shape: (n_layers, batch_size, hidden_dim)\n",
    "        A = (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "       \n",
    "        return A\n",
    "\n",
    "    def forward(self, traj_span):\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(traj_span.view(len(traj_span), 1, -1), \n",
    "                                          self.hidden)\n",
    "        outputs = self.fc(lstm_out.view(len(traj_span), -1))\n",
    "        scores = F.log_softmax(outputs, dim=1)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and testing\n",
    "\n",
    "The following function first choses a model, then trains on train data and finally \n",
    "evaluates the trained model on test data and outputs the accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_data, test_data, hidden_dim=20, num_classes=5, \n",
    "               learning_rate=0.01, n_epochs=100, d=10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    input_dim = len(train_data[0][0][0])\n",
    "    \n",
    "    model = LSTM(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "    loss_function = nn.NLLLoss() #nn.CrossEntropyLoss()\n",
    "    print(\"loss_function=\", loss_function)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"model=\", model)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for sentence, tags in train_data:\n",
    "\n",
    "            # zero the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # zero the hidden state of the LSTM, this detaches it from its history\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # forward pass to get scores\n",
    "            tag_scores = model(sentence)\n",
    "\n",
    "            # computing the loss, and gradients \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # updating the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        if((epoch+1)%d == 0):\n",
    "            print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train)))\n",
    "        \n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for trajs, labels in train_data:\n",
    "        outputs = model(trajs)\n",
    "        n_samples += len(labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100 * n_correct/n_samples\n",
    "    print(\"n_samples=\", n_samples)\n",
    "    print(\"n_correct=\", n_correct)\n",
    "    print(f'train accuracy: {acc}')\n",
    "    \n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for trajs, labels in test_data:\n",
    "        outputs = model(trajs)\n",
    "        n_samples += len(labels)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100 * n_correct/n_samples\n",
    "    print(\"n_samples=\", n_samples)\n",
    "    print(\"n_correct=\", n_correct)\n",
    "    print(f'test accuracy: {acc}')\n",
    "    print(time.time() - start_time)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_1$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping1(data)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= NLLLoss()\n",
      "model= LSTM(\n",
      "  (lstm): LSTM(3, 20)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.00270\n",
      "Epoch: 20, loss: 0.96811\n",
      "Epoch: 30, loss: 0.95783\n",
      "Epoch: 40, loss: 0.95149\n",
      "Epoch: 50, loss: 0.94670\n",
      "Epoch: 60, loss: 0.94417\n",
      "Epoch: 70, loss: 0.94242\n",
      "Epoch: 80, loss: 0.94108\n",
      "Epoch: 90, loss: 0.93997\n",
      "Epoch: 100, loss: 0.93901\n",
      "n_samples= 11694\n",
      "n_correct= 7379\n",
      "train accuracy: 63.10073541987344\n",
      "n_samples= 5839\n",
      "n_correct= 3675\n",
      "test accuracy: 62.9388593937318\n",
      "392.05649185180664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.9388593937318"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim=20, num_classes=5, learning_rate=0.005, \n",
    "           n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_2$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping2(data)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= NLLLoss()\n",
      "model= LSTM(\n",
      "  (lstm): LSTM(7, 200)\n",
      "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.11555\n",
      "Epoch: 20, loss: 1.06255\n",
      "Epoch: 30, loss: 1.04275\n",
      "Epoch: 40, loss: 1.02363\n",
      "Epoch: 50, loss: 1.01422\n",
      "Epoch: 60, loss: 1.00139\n",
      "Epoch: 70, loss: 0.99837\n",
      "Epoch: 80, loss: 0.98935\n",
      "Epoch: 90, loss: 0.98490\n",
      "Epoch: 100, loss: 0.98686\n",
      "n_samples= 11694\n",
      "n_correct= 7018\n",
      "train accuracy: 60.01368223020352\n",
      "n_samples= 5839\n",
      "n_correct= 3516\n",
      "test accuracy: 60.21579037506422\n",
      "626.7157030105591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.21579037506422"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim=200, num_classes=5, learning_rate=0.0008, \n",
    "           n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_3$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping3(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= NLLLoss()\n",
      "model= LSTM(\n",
      "  (lstm): LSTM(20, 100)\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.47024\n",
      "Epoch: 20, loss: 1.45267\n",
      "Epoch: 30, loss: 1.42456\n",
      "Epoch: 40, loss: 1.42089\n",
      "Epoch: 50, loss: 1.40882\n",
      "Epoch: 60, loss: 1.38867\n",
      "Epoch: 70, loss: 1.38180\n",
      "Epoch: 80, loss: 1.39021\n",
      "Epoch: 90, loss: 1.36947\n",
      "Epoch: 100, loss: 1.37255\n",
      "n_samples= 11694\n",
      "n_correct= 4364\n",
      "train accuracy: 37.31828288010946\n",
      "n_samples= 5839\n",
      "n_correct= 2180\n",
      "test accuracy: 37.335160130159274\n",
      "454.395614862442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.335160130159274"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim=100, num_classes=5, learning_rate=0.001, \n",
    "           n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_4$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping4(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= NLLLoss()\n",
      "model= LSTM(\n",
      "  (lstm): LSTM(23, 20)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 0.98205\n",
      "Epoch: 20, loss: 0.97209\n",
      "Epoch: 30, loss: 0.96461\n",
      "Epoch: 40, loss: 0.95818\n",
      "Epoch: 50, loss: 0.95695\n",
      "Epoch: 60, loss: 0.95128\n",
      "Epoch: 70, loss: 0.94829\n",
      "Epoch: 80, loss: 0.94796\n",
      "Epoch: 90, loss: 0.95003\n",
      "Epoch: 100, loss: 0.96687\n",
      "n_samples= 11694\n",
      "n_correct= 7407\n",
      "train accuracy: 63.34017444843509\n",
      "n_samples= 5839\n",
      "n_correct= 3753\n",
      "test accuracy: 64.2747045727008\n",
      "329.3978817462921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.2747045727008"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim=20, num_classes=5, learning_rate=0.01, \n",
    "           n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data using $\\phi_5$ to feed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping5(data, Q)\n",
    "Train, Test = trainTestSplit(Data)\n",
    "\n",
    "train = [0] * len(Train)\n",
    "for i in range(len(Train)):\n",
    "    A = []\n",
    "    train[i] =(torch.from_numpy(Train[i][:,:-1]).float(), \n",
    "               torch.from_numpy(Train[i][:,-1]).long())\n",
    "    \n",
    "test = [0] * len(Test)\n",
    "for i in range(len(Test)):\n",
    "    A = []\n",
    "    test[i] =(torch.from_numpy(Test[i][:,:-1]).float(), \n",
    "              torch.from_numpy(Test[i][:,-1]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_function= NLLLoss()\n",
      "model= LSTM(\n",
      "  (lstm): LSTM(27, 200)\n",
      "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
      ")\n",
      "Epoch: 10, loss: 1.09498\n",
      "Epoch: 20, loss: 1.05340\n",
      "Epoch: 30, loss: 1.03199\n",
      "Epoch: 40, loss: 1.01587\n",
      "Epoch: 50, loss: 1.00371\n",
      "Epoch: 60, loss: 1.00235\n",
      "Epoch: 70, loss: 0.99317\n",
      "Epoch: 80, loss: 0.99613\n",
      "Epoch: 90, loss: 0.98878\n",
      "Epoch: 100, loss: 0.98369\n",
      "n_samples= 11694\n",
      "n_correct= 7401\n",
      "train accuracy: 63.28886608517188\n",
      "n_samples= 5839\n",
      "n_correct= 3712\n",
      "test accuracy: 63.57252954272992\n",
      "664.8755309581757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63.57252954272992"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(train, test, hidden_dim=200, num_classes=5, learning_rate=0.001, \n",
    "           n_epochs=100, d=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
