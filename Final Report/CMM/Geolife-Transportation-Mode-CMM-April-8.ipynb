{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from scipy import linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "from matplotlib import colors\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "from hmmlearn import hmm\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading cleaned csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User i is data_1$[i]$ in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [0] * 57\n",
    "for i in range(len(data_1)):\n",
    "    fnames = glob.glob('labeled csv Geolife/'+str(i)+'/*.csv')\n",
    "    data_1[i] = np.array([np.loadtxt(f, delimiter=',')[1:] for f in fnames])\n",
    "data_1 = np.array(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users are stacked together in data_2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = []\n",
    "fnames = glob.glob('labeled csv Geolife/**/*.csv')\n",
    "for f in fnames:\n",
    "    data_2.append(np.loadtxt(f, delimiter=',')[1:])\n",
    "data_2 = np.array(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40392)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_2[i]) for i in range(len(data_2))])\n",
    "min(A), max(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing segments with length less than 1e-10 because of numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3182"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3 = [0] * len(data_2)\n",
    "h = 1e-10\n",
    "c = 0\n",
    "for i in range(len(data_2)):\n",
    "    p1 = data_2[i][:-1]\n",
    "    p2 = data_2[i][1:]\n",
    "    L = ((p2[:,:2]-p1[:,:2])*(p2[:,:2]-p1[:,:2])).sum(axis =1)\n",
    "    I = np.where(L > h)[0]\n",
    "    J = np.where(L < h)[0]\n",
    "    if len(J) > 0:\n",
    "        c += 1\n",
    "    p1 = p1[I]\n",
    "    p2 = p2[I]\n",
    "    if len(I) == 0:\n",
    "        print(i)\n",
    "    gamma = np.concatenate((p1, p2[-1].reshape(1,4)), 0) \n",
    "    if len(gamma) > 0:\n",
    "        data_3[i] = gamma\n",
    "    data_3[i] = np.array(data_3[i])\n",
    "data_3 = np.array(data_3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 40323, array([1380]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([len(data_3[i]) for i in range(len(data_3))])\n",
    "min(A), max(A), np.where(A > 40000)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning trajectories to less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08333337376825511, 2085.283333301777, (array([2940]),))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 24 * 60 * (days_date('1899/12/30 2:50:06') - days_date('1899/12/30 2:20:06')) == 20 min\n",
    "Time = np.zeros(len(data_3))\n",
    "for i in range(len(data_3)):\n",
    "    Time[i] = 24 * 60 * sum(data_3[i][:,2][1:] - data_3[i][:,2][:-1]) # = 20 minutes \n",
    "min(Time), max(Time), np.where(Time>2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.where(Time>20)[0]\n",
    "len(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(trajectory):\n",
    "    trajectories = []\n",
    "    a = 24 * 60 * sum(trajectory[:,2][1:] - trajectory[:,2][:-1])\n",
    "    if a <= 20:\n",
    "        return np.array(trajectory.reshape(1, len(trajectory), 4))\n",
    "    else: \n",
    "        i = 0\n",
    "        while a > 20:\n",
    "            j = i + 0\n",
    "            val = 0\n",
    "            while val < 20: \n",
    "                if i < len(trajectory) - 1:\n",
    "                    temp = val + 0\n",
    "                    val += 24 * 60 * (trajectory[:,2][1:][i] - trajectory[:,2][:-1][i])\n",
    "                    i += 1\n",
    "                else: \n",
    "                    break\n",
    "            if len(trajectory[j:i-1]) > 0:\n",
    "                trajectories.append(trajectory[j:i-1])\n",
    "            a = a - val\n",
    "        if len(trajectory[i:]) > 0:\n",
    "            trajectories.append(trajectory[i:])\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if partitioning into less than 20 minutes worked correctly\n",
    "for j in J:\n",
    "    A = partition(data_3[j])\n",
    "    B = np.array([24 * 60 * sum(A[i][:,2][1:] - A[i][:,2][:-1]) for i in range(len(A))])\n",
    "    I = np.where(B > 20)[0]\n",
    "    if len(I) > 0: \n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_4 below is the array of trajectories having less than 20 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = []\n",
    "for i in range(len(data_3)):\n",
    "    A = partition(data_3[i])\n",
    "    for j in range(len(A)):\n",
    "        data_4.append(A[j])\n",
    "data_4 = np.array(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11833,), (360, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4.shape, data_4[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.where(np.array([len(data_4[i]) for i in range(len(data_4))]) != 1)[0]\n",
    "data_4 = data_4[I]\n",
    "len(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int1 = np.vectorize(int)\n",
    "data_5 = []\n",
    "c = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) < 2: \n",
    "        data_5.append(data_4[i])\n",
    "        c += 1\n",
    "data_5 = np.array(data_5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_6 = []\n",
    "d = 0\n",
    "for i in range(len(data_4)):\n",
    "    if len(set(int1(data_4[i][:,3]))) == 2: \n",
    "        data_6.append(data_4[i])\n",
    "        d += 1\n",
    "data_6 = np.array(data_6)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a:b\n",
    "# a is the number of labels in a trajectory\n",
    "# b is the number of trajectries with a labels\n",
    "D = {1:10121, 2:1671, 3:39, 4:2, 5:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modes = ['walk', 'bike', 'bus', 'driving', 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trajectories of length 1 with label 0, 1, 2, 3, 4: [3383, 1650, 1929, 2214, 863]\n"
     ]
    }
   ],
   "source": [
    "C = []\n",
    "for j in range(5):\n",
    "    c = 0\n",
    "    for i in range(len(data_5)):\n",
    "        if data_5[i][0][-1] == j:\n",
    "            c += 1\n",
    "    C.append(c)\n",
    "print(\"number of trajectories of length 1 with label 0, 1, 2, 3, 4:\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating trajectories with 3, 4, 5 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing length 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1671, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_7 = [0] * len(data_6)\n",
    "for i in range(len(data_6)):\n",
    "    I = list(set(data_6[i][:,3]))\n",
    "    data_7[i] = []\n",
    "    J1 = np.where(data_6[i][:,3] == I[0])\n",
    "    J2 = np.where(data_6[i][:,3] == I[1])\n",
    "    D1 = data_6[i][J1]\n",
    "    D2 = data_6[i][J2]\n",
    "    data_7[i].append(D1)\n",
    "    data_7[i].append(D2)\n",
    "data_7 = np.array(data_7)\n",
    "data_7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentences of length 3, 4, 5 from 10039 length 1 trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "n_1 = 1000 # number of length 1 sentences\n",
    "n_2 = 1000 # len(data_7)# 1671: number of length 2 sentences\n",
    "n_3 = 800 # number of length 3 sentences\n",
    "n_4 = 700 # number of length 4 sentences\n",
    "n_5 = 1900 # number of length 5 sentences\n",
    "\n",
    "for i in range(n_1):\n",
    "    I = np.random.randint(0, 10039, size=1)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_2): \n",
    "    data.append(data_7[i])\n",
    "\n",
    "for i in range(n_3):\n",
    "    I = np.random.randint(0, 10039, size=3)\n",
    "    data.append(data_5[I])\n",
    "\n",
    "for i in range(n_4):\n",
    "    I = np.random.randint(0, 10039, size=4)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "for i in range(n_5):\n",
    "    I = np.random.randint(0, 10039, size=5)\n",
    "    data.append(data_5[I])\n",
    "    \n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (2,) (3,) (4,) (5,)\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape, data[n_1].shape, data[n_1+n_2].shape, data[n_1+n_2+n_3].shape, \n",
    "      data[n_1+n_2+n_3+n_4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed for CMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "source": [
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "### $\\phi_1(x, y) = (\\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{4n}$\n",
    "\n",
    "### $\\phi_2(x, y) = (\\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{8n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping1(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping $\\phi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping2(data):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature mapping using landmarks\n",
    "\n",
    "$x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "$y = (y_1, y_2, \\ldots, y_n) \\in \\{0,1,2,3,4\\}^n$\n",
    "\n",
    "If $x_i = [(a_0, b_0, t_0), \\ldots, (a_m, b_m, t_m)]$, where $a_j$ is latitude, $b_j$ is longitude and $t_j$ is time, then \n",
    "$$\\displaystyle \\text{length}_i = \\frac{1}{m} \\sum_{j=1}^m \\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2,$$\n",
    "$$\\text{velocity}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j},$$\n",
    "$$\\text{acceleration}_i = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\|(a_j, b_j) - (a_{j-1}, b_{j-1})\\|_2}{t_j^2}.$$\n",
    "\n",
    "#Notice: I have divded the acceleration by 1e10 for all data. \n",
    "\n",
    "#### Now we would like to use a feature mapping introduced in the following paper:\n",
    "\n",
    "``Jeff M. Phillips and Pingfan Tang. Simple distances for trajectories via landmarks. In ACM GIS SIGSPATIAL, 2019.''\n",
    "\n",
    "Following the paper, let $q \\in \\mathbb{R}^2$ be a landmark and $\\gamma$ be a trajectory in $\\mathbb{R}^2$. We define \n",
    "$$v_q(\\gamma) = {\\rm dist}(\\gamma, q) = min_{p \\in \\gamma} \\|q - p\\|_2.$$\n",
    "\n",
    "We randomly choose $m$ (here $m=20$ will be used) landmaks in $\\mathbb{R}^2$ around trajectories and call them $Q$, so $Q=\\{q_1, q_2, \\ldots, q_m\\}$. Then we define the feature mapping $v_Q$ by \n",
    "$$v_Q(\\gamma) = (v_{q_1}(\\gamma), v_{q_2}(\\gamma), \\ldots, v_{q_m}(\\gamma)) \\in \\mathbb{R}^m.$$\n",
    "\n",
    "Then we combine this feature mapping with $\\phi_1$ and $\\phi_2$ to get the following feature mappings:\n",
    "\n",
    "### $\\phi_3(x, y) = (v_Q(x_i), y_i)_{i=1}^n \\in \\mathbb{R}^{(m+1)n}$\n",
    "\n",
    "### $\\phi_4(x, y) = (v_Q(x_i), \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+4)n}$\n",
    "\n",
    "### $\\phi_5(x, y) = (v_Q(x_i), \\text{start point}_i, \\text{end point}_i, \\text{length}_i, \\text{velocity}_i, \\text{acceleration}_i, y_i)_{i=1}^n \\in \\mathbb{R}^{(m+8)n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark Feature Mapping $v_Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap_v_Q(Q, gamma):\n",
    "    \n",
    "    p2 = gamma[1:]\n",
    "    p1 = gamma[:-1]\n",
    "    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))\n",
    "    II = np.where(L>10e-8)[0]\n",
    "    L = L[II]\n",
    "    p1 = p1[II]\n",
    "    p2 = p2[II]\n",
    "    w = (p1-p2)*(-1,1)/(L*np.ones((2,1))).T\n",
    "    w[:,[0, 1]] = w[:,[1, 0]]\n",
    "    \n",
    "    dist_dot = np.sum(w * (Q.reshape(len(Q),1,2) - p1), axis=2)\n",
    "    \n",
    "    x = abs(dist_dot.copy())\n",
    "    R = (L**2).reshape(-1,1)\n",
    "    u = p1 + ((((np.sum(((Q.reshape(len(Q),1,2) - p1) * (p2 - p1)),axis=2).reshape(len(Q)\n",
    "                ,-1,1,1) * (p2-p1).reshape(len(p2-p1),1,2))).reshape(len(Q),len(p1),2))/R)\n",
    "    \n",
    "    G = np.sqrt(np.sum((u-p1)*(u-p1), axis=2))\n",
    "    H = np.sqrt(np.sum((u-p2)*(u-p2), axis=2))\n",
    "    d1 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p1)*(Q.reshape(len(Q),1,2)-p1), axis=2))\n",
    "    d2 = np.sqrt(np.sum((Q.reshape(len(Q),1,2)-p2)*(Q.reshape(len(Q),1,2)-p2), axis=2))\n",
    "\n",
    "    dist = np.where(abs(G + H - L) < np.ones(len(L)) * (10e-8), x, np.minimum(d1, d2))\n",
    "\n",
    "    j = np.argmin(dist, axis =1)\n",
    "    dist_weighted = dist[np.arange(len(dist)),j]\n",
    "    \n",
    "    return dist_weighted.reshape(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureMapping3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping3(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping4(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping composed of $v_Q$ and featureMapping2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMapping5(data, Q):\n",
    "    Data = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        Data[i] = []\n",
    "        for j in range(len(data[i])):\n",
    "            D = data[i][j]\n",
    "            segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "            segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "            I = np.where(segments_lengths > 1e-8)\n",
    "            D = D[I]\n",
    "            if len(D) > 1:\n",
    "                segments = D[:,:2][1:] - D[:,:2][:-1]\n",
    "                segments_lengths = np.sqrt(np.sum((segments)**2, 1))\n",
    "                length = np.sum(segments_lengths)/len(D) + 1e-10\n",
    "                time = D[:,2][1:] - D[:,2][:-1] + 1e-10\n",
    "                velocities = segments_lengths/time\n",
    "                velocity = np.mean(velocities)\n",
    "                accelerations = segments_lengths/time**2\n",
    "                acceleration = np.mean(accelerations)/1e10\n",
    "                y = D[0][-1]\n",
    "                array_vel_acc = np.array([length, velocity, acceleration, \n",
    "                                  D[0][0], D[0][1], D[-1][0], D[-1][1], y])\n",
    "                mapped_v_Q_D = featureMap_v_Q(Q, D[:,:2])\n",
    "                array = np.concatenate((mapped_v_Q_D, array_vel_acc), 0)\n",
    "                Data[i].append(array)\n",
    "        Data[i] = np.array(Data[i])\n",
    "\n",
    "    Data = np.array(Data)\n",
    "    K = np.array([len(Data[i]) for i in range(len(Data))])\n",
    "    J = np.where(K > 0)\n",
    "    Data = Data[J]\n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(data):\n",
    "    I = []\n",
    "    random.shuffle(data)\n",
    "    for j in range(1,6):\n",
    "        I.append(np.where([len(data[i])==j for i in range(len(Data))])[0])\n",
    "\n",
    "    train = np.concatenate((data[I[0]][len(I[0])//3:], data[I[1]][len(I[1])//3:], \n",
    "                            data[I[2]][len(I[2])//3:], data[I[3]][len(I[3])//3:], \n",
    "                            data[I[4]][len(I[4])//3:]), 0)\n",
    "\n",
    "    test = np.concatenate((data[I[0]][:len(I[0])//3], data[I[1]][:len(I[1])//3],\n",
    "                           data[I[2]][:len(I[2])//3], data[I[3]][:len(I[3])//3],\n",
    "                           data[I[4]][:len(I[4])//3]), 0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for feeding to a clsaaifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localModel(data):\n",
    "    X = [0] * 5 # 5 is the length of label set\n",
    "    X[0] = []\n",
    "    for i in range(len(data)):\n",
    "        X[0].append(data[i][0])\n",
    "    X[0] = np.array(X[0])\n",
    "    for j in range(1, 5):\n",
    "        X[j] = []\n",
    "        I = np.where([len(data[i]) > j for i in range(len(data))])[0]\n",
    "        for i in I:\n",
    "            X[j].append(np.insert(data[i][j], len(data[i][j])-1, data[i][j-1][-1], axis=0))\n",
    "        X[j] = np.array(X[j])\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLinearSVM(X, C_list = [1e4, 1e5, 1e5, 1e5, 1e5]): \n",
    "    \n",
    "    clf = [0] * 5 \n",
    "    accuracies = np.zeros(len(X))\n",
    "    for i in range(len(X)): \n",
    "        clf[i] = make_pipeline(LinearSVC(dual=False, C=C_list[i], max_iter = 1000))\n",
    "        clf[i].fit(X[i][:,:-1], X[i][:, -1])\n",
    "        X_pred = clf[i].predict(X[i][:, :-1])\n",
    "        accuracies[i] = metrics.accuracy_score(X[i][:, -1], X_pred)\n",
    "    \n",
    "    print(\"train accuracies for local classifiers given x_i and y_{i-1} =\") \n",
    "    print(accuracies)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGauSVM(X, C_list = [1e4, 1e5, 1e5, 1e5, 1e5], kernel_list = ['rbf'] * 5, \n",
    "                    gamma_list = [1e5] * 5):\n",
    "    \n",
    "    clf = [0] * 5\n",
    "    accuracies = np.zeros(len(X))\n",
    "    for i in range(len(X)): \n",
    "        clf[i] = make_pipeline(svm.SVC(C=C_list[i], kernel = kernel_list[i], \n",
    "                                                gamma= gamma_list[i], max_iter = 200000))\n",
    "        clf[i].fit(X[i][:,:-1], X[i][:, -1])\n",
    "\n",
    "        X_pred = clf[i].predict(X[i][:, :-1])\n",
    "        accuracies[i] = metrics.accuracy_score(X[i][:, -1], X_pred)\n",
    "    \n",
    "    print(\"train accuracies for local classifiers given x_i and y_{i-1} =\") \n",
    "    print(accuracies)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDT(X):\n",
    "    \n",
    "    clf = [0] * 5\n",
    "    accuracies = np.zeros(len(X))\n",
    "\n",
    "    for i in range(len(X)): \n",
    "        clf[i] = DecisionTreeClassifier(random_state=0)\n",
    "        clf[i].fit(X[i][:,:-1], X[i][:, -1])\n",
    "        X_pred = clf[i].predict(X[i][:, :-1])\n",
    "        accuracies[i] = metrics.accuracy_score(X[i][:, -1], X_pred)\n",
    "        \n",
    "    print(\"train accuracies for local classifiers given x_i and y_{i-1} =\") \n",
    "    print(accuracies)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRF(X, num_estimators = 100):\n",
    "    \n",
    "    clf = [0] * 5\n",
    "    accuracies = np.zeros(len(X))\n",
    "\n",
    "    for i in range(len(X)): \n",
    "        clf[i] = RandomForestClassifier(n_estimators=num_estimators, random_state=0)\n",
    "        clf[i].fit(X[i][:,:-1], X[i][:, -1])\n",
    "        X_pred = clf[i].predict(X[i][:, :-1])\n",
    "        accuracies[i] = metrics.accuracy_score(X[i][:, -1], X_pred)\n",
    "        \n",
    "    print(\"train accuracies for local classifiers given x_i and y_{i-1} =\") \n",
    "    print(accuracies)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get scores by the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting scores for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSVM(data, clf):\n",
    "    scores = [0] * len(data)\n",
    "    S_0 = clf[0].decision_function(data[0][:-1].reshape(1,len(data[0])-1))[0]\n",
    "    S_0_arg_added = np.concatenate((-np.ones(5).reshape(-1,1), S_0.reshape(-1,1)), 1)\n",
    "    scores[0] = S_0_arg_added\n",
    "    for i in range(1, len(data)):\n",
    "        S = np.zeros((5,5))\n",
    "        for k in range(5):\n",
    "            a = clf[i].decision_function(np.insert(data[i][:-1], len(data[i])-1,\n",
    "                                            k, axis=0).reshape(1,len(data[i])))[0]\n",
    "            S[:,k] = a + scores[i-1][:,1][k]\n",
    "        scores[i] = np.concatenate((np.argmax(S,1).reshape(-1,1), \n",
    "                                    np.max(S,1).reshape(-1,1)) ,1)\n",
    "    scores = np.array(scores)\n",
    "    return scores\n",
    "\n",
    "def scoresSVM(data, clf):\n",
    "    scores = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        scores[i] = scoreSVM(data[i], clf)\n",
    "    scores = np.array(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting scores for DT and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreDT_RF(data, clf):\n",
    "    scores = [0] * len(data)\n",
    "    S_0 = clf[0].predict_proba(data[0][:-1].reshape(1, len(data[0])-1))[0]\n",
    "    S_0_arg_added = np.concatenate((-np.ones(5).reshape(-1,1), S_0.reshape(-1,1)), 1)\n",
    "    scores[0] = S_0_arg_added\n",
    "    for i in range(1, len(data)):\n",
    "        S = np.zeros((5,5))\n",
    "        for k in range(5):\n",
    "            a = clf[i].predict_proba(np.insert(data[i][:-1], len(data[i])-1,\n",
    "                                                k, axis=0).reshape(1, len(data[i])))[0]\n",
    "            S[:,k] = a + scores[i-1][:,1][k]\n",
    "        scores[i] = np.concatenate((np.argmax(S,1).reshape(-1,1), \n",
    "                                    np.max(S,1).reshape(-1,1)) ,1)\n",
    "    scores = np.array(scores)\n",
    "    return scores\n",
    "\n",
    "def scoresDT_RF(data, clf):\n",
    "    scores = [0] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        scores[i] = scoreDT_RF(data[i], clf)\n",
    "    scores = np.array(scores)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi algorithm (in order to find argmaxs i.e., labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictViterbiSVM(data, clf):\n",
    "    scores = scoresSVM(data, clf)\n",
    "    y_pred = [0] * len(data)\n",
    "    for j in range(len(data)):\n",
    "        if len(data[j]) > 1:\n",
    "            y_pred[j] = np.zeros(len(data[j]))\n",
    "            b = int(np.argmax(scores[j][-1][:,1]))\n",
    "            y_pred[j][-1] = b\n",
    "            for i in range(len(data[j])-1, 0, -1):\n",
    "                b = int(scores[j][i][b][0])\n",
    "                y_pred[j][i-1] = b\n",
    "        elif len(data[j]) == 1:\n",
    "            y_pred[j] = np.zeros(len(data[j]))\n",
    "            y_pred[j][0] = int(scores[j][0][0][1])\n",
    "\n",
    "    accuracies = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        accuracies[i] = metrics.accuracy_score(y_pred[i], data[i][:,-1])\n",
    "    accuracy = np.mean(accuracies)\n",
    "    return y_pred, accuracy, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi for DT and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictViterbiDT_RF(data, clf):\n",
    "    scores = scoresDT_RF(data, clf)\n",
    "    y_pred = [0] * len(data)\n",
    "    for j in range(len(data)):\n",
    "        if len(data[j]) > 1:\n",
    "            y_pred[j] = np.zeros(len(data[j]))\n",
    "            b = int(np.argmax(scores[j][-1][:,1]))\n",
    "            y_pred[j][-1] = b\n",
    "            for i in range(len(data[j])-1, 0, -1):\n",
    "                b = int(scores[j][i][b][0])\n",
    "                y_pred[j][i-1] = b\n",
    "        elif len(data[j]) == 1:\n",
    "            y_pred[j] = np.zeros(len(data[j]))\n",
    "            y_pred[j][0] = int(np.argmax(scores[j][-1][:,1]))\n",
    "\n",
    "    accuracies = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        accuracies[i] = metrics.accuracy_score(y_pred[i], data[i][:,-1])\n",
    "    accuracy = np.mean(accuracies)\n",
    "    return y_pred, accuracy, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy way of choosing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictPoint(data, clf): # data is a point of data\n",
    "    s = 0\n",
    "    y_pred = [0] * len(data)\n",
    "    y_pred[0] = int(clf[0].predict(data[0][:-1].reshape(1,len(data[0])-1))[0])\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        b = int(clf[i].predict(np.insert(data[i][:-1], len(data[i])-1, y_pred[i-1], \n",
    "                                         axis=0).reshape(1,len(data[i])))[0])\n",
    "\n",
    "        y_pred[i] = b\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_pred, data[:,-1])\n",
    "    return y_pred, accuracy\n",
    "\n",
    "def predict(data, clf):\n",
    "    y_pred = [0] * len(data)\n",
    "    accuracies = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        A = predictPoint(data[i], clf)\n",
    "        y_pred[i] = A[0]\n",
    "        accuracies[i] = A[1]\n",
    "    return y_pred, np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience with feature mapping $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping1(data)\n",
    "train, test = trainTestSplit(Data)\n",
    "X = localModel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.56785317 0.52866022 0.49291408 0.50756694 0.51701783]\n",
      "train accuracy = 0.5110585836114201\n",
      "test accuracy = 0.5042711234911792\n"
     ]
    }
   ],
   "source": [
    "clf = trainLinearSVM(X, C_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.70105673 0.74343923 0.75819309 0.78987194 0.82901135]\n",
      "train accuracy = 0.5729282536151279\n",
      "test accuracy = 0.48201485608170846\n"
     ]
    }
   ],
   "source": [
    "clf = trainGauSVM(X, C_list = [1e5]*5, kernel_list = ['rbf']*5, gamma_list = [10]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.70105673 0.74343923 0.75819309 0.78987194 0.82901135]\n",
      "train accuracy = 0.6949573600296626\n",
      "test accuracy = 0.584614670380687\n"
     ]
    }
   ],
   "source": [
    "clf = trainClassifier(X, C_list = [1e5]*5, kernel_list = ['rbf']*5, gamma_list = [10]*5)\n",
    "print(\"train accuracy =\", predict(train, clf)[1])\n",
    "print(\"test accuracy =\", predict(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1. 1. 1. 1. 1.]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.6743782479584262\n"
     ]
    }
   ],
   "source": [
    "clfTree = trainDT(X)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfTree)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfTree)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.9585881  0.97582873 0.97918512 0.97322468 0.97317073]\n",
      "train accuracy = 0.9696266444320919\n",
      "test accuracy = 0.7205270972531551\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 10)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9864710789766407\n",
      "test accuracy = 0.7221448467966574\n",
      "train accuracy = 0.9833611420096404\n",
      "test accuracy = 0.7256917363045498\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 20)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9918149796069707\n",
      "test accuracy = 0.7310492107706592\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 100)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9928485354097146\n",
      "test accuracy = 0.7311420612813371\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 200)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience with feature mapping $\\phi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping2(data)\n",
    "train, test = trainTestSplit(Data)\n",
    "X = localModel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.57559755 0.51208564 0.51062888 0.53899884 0.54227642]\n",
      "train accuracy = 0.5162451361867705\n",
      "test accuracy = 0.5101707498144024\n"
     ]
    }
   ],
   "source": [
    "clf = trainLinearSVM(X, C_list = [0.01]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.76223582 0.74171271 0.75775022 0.7677532  0.82090762]\n",
      "train accuracy = 0.6226362625139044\n",
      "test accuracy = 0.5474558960074281\n"
     ]
    }
   ],
   "source": [
    "clff = trainGauSVM(X, C_list = [1e4]*5, kernel_list = ['rbf']*5, gamma_list = [0.1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clff)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clff)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 1.0\n",
      "test accuracy = 0.7374094707520892\n"
     ]
    }
   ],
   "source": [
    "clfTree = trainDT(X)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfTree)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfTree)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.98943858 0.98825967 0.99069973 0.98719441 0.9902439 ]\n",
      "train accuracy = 0.9895034278302761\n",
      "test accuracy = 0.79318856718634\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 10)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9970198368557656\n",
      "test accuracy = 0.8093314763231197\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 20)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 1.0\n",
      "test accuracy = 0.8208542246982359\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 100)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 1.0\n",
      "test accuracy = 0.8198514391829155\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 200)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience with feature mapping $v_Q$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, c = np.min([np.min([np.min(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "  \n",
    "b, d = np.max([np.max([np.max(data[i][j][:,:2], axis=0) for j in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Mean = np.mean([np.mean([np.mean(data[i][k][:,:2], axis=0) for k in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "Std = np.std([np.std([np.std(data[i][l][:,:2], axis=0) for l in range(len(data[i]))], \n",
    "                      axis=0) for i in range(len(data))], axis=0)\n",
    "\n",
    "m = 20\n",
    "Q = np.ones((m,2))\n",
    "\n",
    "Q[:,0] = np.random.normal(Mean[0], 100*Std[0], m)\n",
    "Q[:,1] = np.random.normal(Mean[1], 20*Std[1], m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    plt.plot(data[i][0][:,0], data[i][0][:,1]);\n",
    "\n",
    "plt.scatter(Q[:,0], Q[:,1], s = 10, color = \"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience with feature mapping $\\phi_3 = v_Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping3(data, Q)\n",
    "train, test = trainTestSplit(Data)\n",
    "X = localModel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.48832036 0.35773481 0.37987589 0.41860465 0.38673139]\n",
      "train accuracy = 0.4276140155728587\n",
      "test accuracy = 0.4290553080920564\n"
     ]
    }
   ],
   "source": [
    "clf = trainLinearSVM(X, C_list = [1e8]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.70828699 0.63639503 0.68572695 0.71627907 0.72330097]\n",
      "train accuracy = 0.5466629588431591\n",
      "test accuracy = 0.48827951002227177\n"
     ]
    }
   ],
   "source": [
    "clf = trainGauSVM(X, C_list = [1e4]*5, kernel_list = ['rbf']*5, gamma_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1. 1. 1. 1. 1.]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.6825909428359317\n"
     ]
    }
   ],
   "source": [
    "clfTree = trainDT(X)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfTree)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfTree)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.99388209 0.99551105 0.99734043 0.99476744 0.99190939]\n",
      "train accuracy = 0.9949295513533555\n",
      "test accuracy = 0.7244988864142538\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 20)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1.         1.         0.99955674 1.         1.        ]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.729370824053452\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 100)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience with feature mapping $\\phi_4 = v_Q$ + $\\phi_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping4(data, Q)\n",
    "train, test = trainTestSplit(Data)\n",
    "X = localModel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.58866037 0.52555249 0.54162976 0.53667055 0.52845528]\n",
      "train accuracy = 0.5218732629238465\n",
      "test accuracy = 0.5281644394951744\n"
     ]
    }
   ],
   "source": [
    "clf = trainLinearSVM(X, C_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.8627015  0.89779006 0.93312666 0.94703143 0.96178862]\n",
      "train accuracy = 0.7336714841578654\n",
      "test accuracy = 0.5287397921306607\n"
     ]
    }
   ],
   "source": [
    "clf = trainGauSVM(X, C_list = [1e4]*5, kernel_list = ['rbf']*5, gamma_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1. 1. 1. 1. 1.]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.7343541202672607\n"
     ]
    }
   ],
   "source": [
    "clfTree = trainDT(X)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfTree)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfTree)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.99638688 0.99585635 0.99734278 0.99708964 0.99756098]\n",
      "train accuracy = 0.9960857884009634\n",
      "test accuracy = 0.8237286562731996\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 20)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1.         1.         1.         0.99941793 1.        ]\n",
      "train accuracy = 0.9999444135630906\n",
      "test accuracy = 0.8316536748329622\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 100)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience with feature mapping $\\phi_5 = v_Q$ + $\\phi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = featureMapping5(data, Q)\n",
    "train, test = trainTestSplit(Data)\n",
    "X = localModel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.59505281 0.5176105  0.54517272 0.55529686 0.55365854]\n",
      "train accuracy = 0.5279043913285159\n",
      "test accuracy = 0.514355976243504\n"
     ]
    }
   ],
   "source": [
    "clf = trainLinearSVM(X, C_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=200000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.88966092 0.92921271 0.94641275 0.97380675 0.97235772]\n",
      "train accuracy = 0.7554243098017417\n",
      "test accuracy = 0.525231997030438\n"
     ]
    }
   ],
   "source": [
    "clf = trainGauSVM(X, C_list = [1e4]*5, kernel_list = ['rbf']*5, gamma_list = [1]*5)\n",
    "print(\"train accuracy =\", predictViterbiSVM(train, clf)[1])\n",
    "print(\"test accuracy =\", predictViterbiSVM(test, clf)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1. 1. 1. 1. 1.]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.7451837416481069\n"
     ]
    }
   ],
   "source": [
    "clfTree = trainDT(X)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfTree)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfTree)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[0.99749861 0.99551105 0.99689991 0.99534342 0.99756098]\n",
      "train accuracy = 0.9963915138039651\n",
      "test accuracy = 0.8228841870824054\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 20)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies for local classifiers given x_i and y_{i-1} =\n",
      "[1. 1. 1. 1. 1.]\n",
      "train accuracy = 1.0\n",
      "test accuracy = 0.8276818856718634\n"
     ]
    }
   ],
   "source": [
    "clfRF = trainRF(X, 100)\n",
    "print(\"train accuracy =\", predictViterbiDT_RF(train, clfRF)[1])\n",
    "print(\"test accuracy =\", predictViterbiDT_RF(test, clfRF)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
